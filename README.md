# Project: Data Warehouse
In this project we will create a data warehouse on AWS and build an ETL pipeline for a database hosted on Redshift.  To complete the project, you will need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## Data Sets
The data will reside in S3, links will be below.

#### Song Data
The first dataset is a subset of real data from the Million Song Dataset(http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song.
- Link: s3://udacity-dend/song_data

#### Log Data
The second dataset consists of log files in JSON format generated by this event simulator(https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.
- Link: s3://udacity-dend/log_data
- Json path: s3://udacity-dend/log_json_path.json

## Tables
The song and log data above will be used to create the tables below on our data warehouse.

#### Fact Table
- songplays - records in log data associated with song plays i.e. records with page NextSong
    - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
    
#### Dimension Tables

- users - users in the app
    - user_id, first_name, last_name, gender, level
- songs - songs in music database
    - song_id, title, artist_id, year, duration
- artists - artists in music database
    - artist_id, name, location, latitude, longitude
- time - timestamps of records in songplays broken down into specific units
    - start_time, hour, day, week, month, year, weekday

## Usage
This will outline the steps to create and delete the data warehouse.

- Open `dwh.cgf` and input your AWS key and secret key into the [AWS] section
- Create cluster, get Endpoint and IAM role:
    - Run `create_cluster.py` to set up AWS cluster and IAM role.
    - When cluster if available run `get_endpoint_iamrole.py`.
    - Or
    - Run `create_cluster.ipynb` to set up ASW cluster, IAM role, and get Endpoint and IAM role.
    
- Copy Endpoint and IAM role into `dwh.cfg`.
- Run `create_tables.py` to create tables in data warehouse.
- Run `etl.py` load S3 data, transform, and load data into tables.
- Run `close_cluster.py` to close cluster and remove IAM role.

## Files

- 'sql_queries.py': This python program contains all sql queries used in all other files.
- 'dwh.cfg': This file contains specifications for the data warehouse.
- `create_cluster.ipynb`: This Jupyter notebook goes through the process of creating the AWS cluster, IAM role, and attaching policy to the role.
- `create_cluster.py`: This python program will create the AWS cluster and IAM role.
- `get_endpoint_iamrole.py`: This python program will return the Endpoint and IAM role if the cluster is available.
- 'create_tables.py': This python program will delete and create tables on our Redshift cluster.
- 'etl.ipynb': This Jupyter notebook goes through the process of setting up the queries that will be used in `etl.py`.
- `etl.py`: This python program will load the S3 data and transform the data into tables on our Redshift cluster.
- `close_cluster.ipynb`: This Jupyter notebook goes throught the process of setting up `close_cluster.py`
- `close_cluster.py`: This python program will delete the cluster and remove the iam role.


###### Author

Ryan Mezera